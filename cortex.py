import sys
import json
import asyncio
import aiohttp
import os
from pathlib import Path
import yaml
from datetime import datetime
from PySide6.QtWidgets import (
    QApplication,
    QMainWindow,
    QVBoxLayout,
    QHBoxLayout,
    QWidget,
    QTextEdit,
    QLineEdit,
    QPushButton,
    QComboBox,
    QLabel,
    QSplitter,
    QProgressBar,
    QTabWidget,
    QFrame,
    QGroupBox,
    QGridLayout,
    QListWidget,
    QListWidgetItem,
    QMessageBox,
    QDialog,
    QDialogButtonBox,
    QScrollArea,
)
from PySide6.QtCore import Qt, QThread, Signal, QTimer
from PySide6.QtGui import QFont, QTextCursor, QPalette
from PySide6.QtWebEngineWidgets import QWebEngineView
import markdown
from markdown.extensions import codehilite, fenced_code, tables
import markdown.extensions.extra
import re

from query_analyzer import IterativeChatManager, QueryAnalysisResult

class ConfigManager:
    """YAML-based configuration management"""

    def __init__(self):
        self.config_dir = Path.home() / ".cortex"
        self.config_file = self.config_dir / "config.yaml"
        self.workspaces_dir = self.config_dir / "workspaces"
        self.conversations_dir = self.config_dir / "conversations"

        # Create directories if they don't exist
        self.config_dir.mkdir(exist_ok=True)
        self.workspaces_dir.mkdir(exist_ok=True)
        self.conversations_dir.mkdir(exist_ok=True)

        self.config = self.load_config()

    def load_config(self):
        """Load configuration from YAML file"""
        default_config = {
            "window": {"width": 1400, "height": 900, "x": 100, "y": 100},
            "ollama": {
                "url": "http://localhost:11434",
                "timeout": 30,
                "retry_attempts": 3,
                "models": {
                    "chat": "",
                    "search_extract": "",
                    "search_summarize": "",
                    "code_analyze": "",
                    "code_summarize": "",
                    "embedding": "nomic-embed-text",
                },
            },
            "ui": {
                "stream_enabled": True,
                "font_size": 10,
                "theme": "default",
                "show_raw_tab": True,
                "show_debug_tab": True,
                "auto_save_conversations": True,
            },
            "prompts": {
                "chat": "You are a helpful assistant.",
                "search_extract": "Extract relevant information from the following web search results for the query: {query}",
                "search_summarize": "Summarize the following extracted information into a coherent response: {content}",
                "code_analyze": "Analyze the following code and explain its functionality: {code}",
                "code_summarize": "Provide a hierarchical summary of the following codebase: {content}",
            },
            "workspaces": {"current": "default", "list": ["default"]},
            "search": {
                "enabled": False,
                "providers": {"brave": {"enabled": False, "api_key": ""}, "duckduckgo": {"enabled": True}},
                "max_results": 10,
                "timeout": 15,
            },
            "rag": {"chunk_size": 1000, "chunk_overlap": 200, "max_context_length": 4000, "rerank_enabled": True},
        }

        if self.config_file.exists():
            try:
                with open(self.config_file, "r", encoding="utf-8") as f:
                    loaded_config = yaml.safe_load(f) or {}
                return self._deep_merge(default_config, loaded_config)
            except (yaml.YAMLError, IOError) as e:
                print(f"Warning: Could not load config file: {e}")
                return default_config
        else:
            self._create_initial_config()
            return default_config

    def _deep_merge(self, default, loaded):
        """Deep merge two dictionaries"""
        result = default.copy()
        for key, value in loaded.items():
            if key in result and isinstance(result[key], dict) and isinstance(value, dict):
                result[key] = self._deep_merge(result[key], value)
            else:
                result[key] = value
        return result

    def _create_initial_config(self):
        """Create initial config file with helpful comments"""
        yaml_content = """# Cortex RAG Agent Configuration
# This file is automatically generated but can be safely edited

# Window settings
window:
  width: 1400
  height: 900
  x: 100
  y: 100

# Ollama connection settings  
ollama:
  url: http://localhost:11434
  timeout: 30
  retry_attempts: 3
  models:
    chat: ""                    # Main chat model
    search_extract: ""          # Web search information extraction
    search_summarize: ""        # Web search result summarization
    code_analyze: ""            # Code analysis and explanation
    code_summarize: ""          # Codebase hierarchical summarization
    embedding: nomic-embed-text # Embedding model for RAG

# User interface preferences
ui:
  stream_enabled: true
  font_size: 10
  theme: default
  show_raw_tab: true
  show_debug_tab: true
  auto_save_conversations: true

# System prompts for different tasks
prompts:
  chat: "You are a helpful assistant."
  search_extract: "Extract relevant information from the following web search results for the query: {query}"
  search_summarize: "Summarize the following extracted information into a coherent response: {content}"
  code_analyze: "Analyze the following code and explain its functionality: {code}"
  code_summarize: "Provide a hierarchical summary of the following codebase: {content}"

# Workspace management
workspaces:
  current: default
  list:
    - default

# Web search configuration
search:
  enabled: false
  providers:
    brave:
      enabled: false
      api_key: ""  # Get from https://brave.com/search/api/
    duckduckgo:
      enabled: true
  max_results: 10
  timeout: 15

# RAG system settings
rag:
  chunk_size: 1000
  chunk_overlap: 200
  max_context_length: 4000
  rerank_enabled: true
"""
        try:
            with open(self.config_file, "w", encoding="utf-8") as f:
                f.write(yaml_content)
        except IOError as e:
            print(f"Warning: Could not create initial config file: {e}")

    def save_config(self):
        """Save configuration to YAML file"""
        try:
            with open(self.config_file, "w", encoding="utf-8") as f:
                yaml.dump(self.config, f, default_flow_style=False, indent=2, allow_unicode=True, sort_keys=False)
        except IOError as e:
            print(f"Warning: Could not save config file: {e}")

    def get(self, key_path, default=None):
        """Get configuration value using dot notation"""
        keys = key_path.split(".")
        value = self.config

        for key in keys:
            if isinstance(value, dict) and key in value:
                value = value[key]
            else:
                return default

        return value

    def set(self, key_path, value):
        """Set configuration value using dot notation"""
        keys = key_path.split(".")
        config = self.config

        for key in keys[:-1]:
            if key not in config or not isinstance(config[key], dict):
                config[key] = {}
            config = config[key]

        config[keys[-1]] = value

    def save_conversation(self, messages, model_used):
        """Save conversation to file"""
        timestamp = datetime.now().isoformat()
        filename = f"conversation_{timestamp.replace(':', '-')[:19]}.yaml"
        filepath = self.conversations_dir / filename

        conversation_data = {"timestamp": timestamp, "model": model_used, "messages": messages}

        try:
            with open(filepath, "w", encoding="utf-8") as f:
                yaml.dump(conversation_data, f, default_flow_style=False, allow_unicode=True)
            return filepath
        except IOError as e:
            print(f"Warning: Could not save conversation: {e}")
            return None

    def load_conversations(self):
        """Load all conversations"""
        conversations = []
        try:
            for file_path in self.conversations_dir.glob("conversation_*.yaml"):
                with open(file_path, "r", encoding="utf-8") as f:
                    data = yaml.safe_load(f)
                    data["filename"] = file_path.name
                    conversations.append(data)
        except Exception as e:
            print(f"Warning: Could not load conversations: {e}")

        return sorted(conversations, key=lambda x: x["timestamp"], reverse=True)


class OllamaStatusChecker(QThread):
    """Worker thread for checking Ollama status and fetching models"""

    status_changed = Signal(bool, str)
    models_updated = Signal(list)
    connection_lost = Signal()

    def __init__(self):
        super().__init__()
        self.running = True
        self.ollama_base_url = "http://localhost:11434"
        self.has_models = False
        self.poll_continuously = True

    async def check_status_and_models(self):
        """Check if Ollama is running and fetch available models"""
        try:
            async with aiohttp.ClientSession() as session:
                # Check if Ollama is running
                async with session.get(
                    f"{self.ollama_base_url}/api/version", timeout=aiohttp.ClientTimeout(total=3)
                ) as response:
                    if response.status == 200:
                        # Only fetch models if we don't have them or if forced
                        if not self.has_models or self.poll_continuously:
                            try:
                                async with session.get(f"{self.ollama_base_url}/api/tags") as models_response:
                                    if models_response.status == 200:
                                        data = await models_response.json()
                                        models = [model["name"] for model in data.get("models", [])]
                                        self.models_updated.emit(models)
                                        self.has_models = True
                                        self.poll_continuously = False  # Stop continuous polling
                                        self.status_changed.emit(True, f"Ollama running - {len(models)} models available")
                                    else:
                                        self.status_changed.emit(True, "Ollama running - models list unavailable")
                            except Exception:
                                self.status_changed.emit(True, "Ollama running - models list error")
                        else:
                            # Just confirm it's still running without fetching models
                            self.status_changed.emit(True, "Ollama running")
                    else:
                        self.has_models = False
                        self.poll_continuously = True
                        self.status_changed.emit(False, f"Ollama responded with status {response.status}")
        except Exception:
            self.has_models = False
            self.poll_continuously = True
            self.connection_lost.emit()
            self.status_changed.emit(False, "Ollama not responding")

    def force_model_refresh(self):
        """Force a model list refresh"""
        self.poll_continuously = True

    def run(self):
        """Run the status check loop"""
        while self.running:
            asyncio.run(self.check_status_and_models())
            # Longer interval when we have models and connection is stable
            sleep_time = 10000 if self.has_models and not self.poll_continuously else 5000
            self.msleep(sleep_time)

    def stop(self):
        """Stop the status checker"""
        self.running = False


class ModelConfigDialog(QDialog):
    """Dialog for configuring models for different tasks"""

    def __init__(self, config_manager, available_models, parent=None):
        super().__init__(parent)
        self.config_manager = config_manager
        self.available_models = available_models
        self.model_combos = {}
        self.setup_ui()

    def setup_ui(self):
        """Setup the dialog UI"""
        self.setWindowTitle("Model Configuration")
        self.setModal(True)
        self.resize(600, 400)

        layout = QVBoxLayout(self)

        # Scroll area for model configurations
        scroll = QScrollArea()
        scroll_widget = QWidget()
        scroll_layout = QVBoxLayout(scroll_widget)

        # Model tasks
        tasks = [
            ("chat", "Main Chat Model", "Primary model for general conversation"),
            ("search_extract", "Search Extraction", "Extract relevant info from web search results"),
            ("search_summarize", "Search Summarization", "Summarize extracted search information"),
            ("code_analyze", "Code Analysis", "Analyze and explain code functionality"),
            ("code_summarize", "Code Summarization", "Create hierarchical code summaries"),
            ("embedding", "Embedding Model", "Vector embeddings for RAG (typically nomic-embed-text)"),
        ]

        for task_key, task_name, task_desc in tasks:
            group = QGroupBox(task_name)
            group_layout = QVBoxLayout(group)

            # Description
            desc_label = QLabel(task_desc)
            desc_label.setStyleSheet("color: #666; font-style: italic;")
            desc_label.setWordWrap(True)
            group_layout.addWidget(desc_label)

            # Model selection
            combo = QComboBox()
            combo.addItem("-- No model selected --", "")

            current_model = self.config_manager.get(f"ollama.models.{task_key}", "")
            current_index = 0

            for i, model in enumerate(self.available_models, 1):
                combo.addItem(model, model)
                if model == current_model:
                    current_index = i

            combo.setCurrentIndex(current_index)
            self.model_combos[task_key] = combo

            group_layout.addWidget(combo)
            scroll_layout.addWidget(group)

        scroll.setWidget(scroll_widget)
        layout.addWidget(scroll)

        # Buttons
        buttons = QDialogButtonBox(QDialogButtonBox.Ok | QDialogButtonBox.Cancel)
        buttons.accepted.connect(self.accept)
        buttons.rejected.connect(self.reject)
        layout.addWidget(buttons)

    def get_selected_models(self):
        """Get the selected models"""
        return {task: combo.currentData() for task, combo in self.model_combos.items()}


class PromptConfigDialog(QDialog):
    """Dialog for configuring prompts for different tasks"""

    def __init__(self, config_manager, parent=None):
        super().__init__(parent)
        self.config_manager = config_manager
        self.prompt_editors = {}
        self.setup_ui()

    def setup_ui(self):
        """Setup the dialog UI"""
        self.setWindowTitle("Prompt Configuration")
        self.setModal(True)
        self.resize(800, 600)

        layout = QVBoxLayout(self)

        # Tab widget for different prompts
        tabs = QTabWidget()

        prompts = [
            ("chat", "Chat", "Main conversation prompt"),
            ("search_extract", "Search Extract", "Web search information extraction"),
            ("search_summarize", "Search Summarize", "Web search result summarization"),
            ("code_analyze", "Code Analysis", "Code analysis and explanation"),
            ("code_summarize", "Code Summary", "Codebase hierarchical summarization"),
        ]

        for prompt_key, tab_name, description in prompts:
            tab_widget = QWidget()
            tab_layout = QVBoxLayout(tab_widget)

            # Description
            desc_label = QLabel(description)
            desc_label.setStyleSheet("font-weight: bold; color: #333;")
            tab_layout.addWidget(desc_label)

            # Prompt editor
            editor = QTextEdit()
            editor.setPlainText(self.config_manager.get(f"prompts.{prompt_key}", ""))
            editor.setFont(QFont("Consolas", 10))
            self.prompt_editors[prompt_key] = editor
            tab_layout.addWidget(editor)

            # Available variables info
            if "{" in self.config_manager.get(f"prompts.{prompt_key}", ""):
                info_label = QLabel("Available variables: {query}, {content}, {code}")
                info_label.setStyleSheet("color: #666; font-style: italic;")
                tab_layout.addWidget(info_label)

            tabs.addTab(tab_widget, tab_name)

        layout.addWidget(tabs)

        # Buttons
        buttons = QDialogButtonBox(QDialogButtonBox.Ok | QDialogButtonBox.Cancel)
        buttons.accepted.connect(self.accept)
        buttons.rejected.connect(self.reject)
        layout.addWidget(buttons)

    def get_prompts(self):
        """Get the edited prompts"""
        return {prompt: editor.toPlainText() for prompt, editor in self.prompt_editors.items()}
    
class OllamaWorker(QThread):
    """Worker thread for Ollama API communication"""

    response_received = Signal(str)
    error_occurred = Signal(str)
    stream_chunk = Signal(str)

    def __init__(self, model, message, stream=True, system_prompt=""):
        super().__init__()
        self.model = model
        self.message = message
        self.stream = stream
        self.system_prompt = system_prompt
        self.ollama_url = "http://localhost:11434/api/chat"

    async def send_request(self):
        """Send request to Ollama API"""
        messages = []
        if self.system_prompt:
            messages.append({"role": "system", "content": self.system_prompt})
        messages.append({"role": "user", "content": self.message})

        payload = {"model": self.model, "messages": messages, "stream": self.stream}

        try:
            async with aiohttp.ClientSession() as session:
                async with session.post(self.ollama_url, json=payload) as response:
                    if response.status == 200:
                        if self.stream:
                            full_response = ""
                            async for line in response.content:
                                if line:
                                    try:
                                        data = json.loads(line.decode("utf-8"))
                                        if "message" in data and "content" in data["message"]:
                                            chunk = data["message"]["content"]
                                            full_response += chunk
                                            self.stream_chunk.emit(chunk)
                                        if data.get("done", False):
                                            self.response_received.emit(full_response)
                                            break
                                    except json.JSONDecodeError:
                                        continue
                        else:
                            data = await response.json()
                            if "message" in data and "content" in data["message"]:
                                self.response_received.emit(data["message"]["content"])
                    else:
                        self.error_occurred.emit(f"HTTP {response.status}: {await response.text()}")
        except Exception as e:
            self.error_occurred.emit(f"Connection error: {str(e)}")

    def run(self):
        """Run the async request in thread"""
        asyncio.run(self.send_request())


class IterativeOllamaWorker(QThread):
    """Enhanced worker thread for iterative chat processing"""

    response_received = Signal(str)
    error_occurred = Signal(str)
    stream_chunk = Signal(str)
    status_updated = Signal(str)  # New signal for status updates
    analysis_completed = Signal(dict)  # Signal when analysis is complete

    def __init__(self, config_manager, user_query, conversation_context=None):
        super().__init__()
        self.config_manager = config_manager
        self.user_query = user_query
        self.conversation_context = conversation_context or []

        # Create the iterative chat manager
        self.chat_manager = IterativeChatManager(config_manager, status_callback=self.status_updated.emit)

    async def process_query(self):
        """Process the query through the iterative system"""
        try:
            result = await self.chat_manager.process_query(self.user_query, self.conversation_context)

            # Emit the analysis for UI display
            self.analysis_completed.emit(
                {
                    "analysis": result["analysis"],
                    "search_results": result["search_results"],
                    "steps": result["steps_performed"],
                }
            )

            # Emit the final response
            self.response_received.emit(result["response"])

        except Exception as e:
            self.error_occurred.emit(f"Processing error: {str(e)}")

    def run(self):
        """Run the async processing in thread"""
        asyncio.run(self.process_query())


class AnalysisDisplayWidget(QWidget):
    """Widget to display query analysis and steps"""

    def __init__(self):
        super().__init__()
        self.setup_ui()

    def setup_ui(self):
        layout = QVBoxLayout(self)

        # Analysis section
        analysis_group = QGroupBox("Query Analysis")
        analysis_layout = QVBoxLayout(analysis_group)

        self.reasoning_label = QLabel("No analysis yet")
        self.reasoning_label.setWordWrap(True)
        self.reasoning_label.setStyleSheet("font-style: italic; color: #666;")
        analysis_layout.addWidget(QLabel("Reasoning:"))
        analysis_layout.addWidget(self.reasoning_label)

        self.confidence_label = QLabel("Confidence: N/A")
        self.confidence_label.setStyleSheet("font-weight: bold;")
        analysis_layout.addWidget(self.confidence_label)

        layout.addWidget(analysis_group)

        # Steps section
        steps_group = QGroupBox("Processing Steps")
        steps_layout = QVBoxLayout(steps_group)

        self.steps_text = QTextEdit()
        self.steps_text.setMaximumHeight(150)
        self.steps_text.setReadOnly(True)
        self.steps_text.setFont(QFont("Consolas", 9))
        steps_layout.addWidget(self.steps_text)

        layout.addWidget(steps_group)

        # Current status
        self.status_label = QLabel("Ready")
        self.status_label.setStyleSheet("color: #0066cc; font-weight: bold;")
        layout.addWidget(self.status_label)

    def update_analysis(self, analysis_data):
        """Update the analysis display"""
        analysis = analysis_data["analysis"]

        # Update reasoning
        self.reasoning_label.setText(analysis.reasoning)

        # Update confidence with color coding
        confidence = analysis.confidence
        self.confidence_label.setText(f"Confidence: {confidence:.1%}")

        if confidence >= 0.8:
            color = "green"
        elif confidence >= 0.5:
            color = "orange"
        else:
            color = "red"
        self.confidence_label.setStyleSheet(f"font-weight: bold; color: {color};")

        # Update steps
        self.steps_text.setPlainText(analysis_data["steps"])

    def update_status(self, status):
        """Update the current status"""
        self.status_label.setText(status)


class MarkdownDisplay(QWebEngineView):
    """Widget for displaying markdown content with LaTeX support"""

    def __init__(self):
        super().__init__()
        self.setMinimumHeight(300)
        self.current_content = ""

        # Configure markdown processor
        self.md = markdown.Markdown(
            extensions=["extra", "codehilite", "fenced_code", "toc"],
            extension_configs={"codehilite": {"css_class": "highlight", "use_pygments": True}},
        )

        # CSS for styling
        self.css = """
        <style>
            body {
                font-family: 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif;
                line-height: 1.6;
                color: #333;
                max-width: none;
                margin: 0;
                padding: 20px;
                background-color: #fafafa;
            }
            code {
                background-color: #f4f4f4;
                padding: 2px 4px;
                border-radius: 3px;
                font-family: 'Consolas', 'Courier New', monospace;
            }
            pre {
                background-color: #f8f8f8;
                border: 1px solid #e1e1e1;
                border-radius: 4px;
                padding: 10px;
                overflow-x: auto;
            }
            blockquote {
                border-left: 4px solid #ddd;
                margin: 0;
                padding-left: 20px;
                color: #666;
            }
            table {
                border-collapse: collapse;
                width: 100%;
                margin: 10px 0;
            }
            th, td {
                border: 1px solid #ddd;
                padding: 8px;
                text-align: left;
            }
            th {
                background-color: #f2f2f2;
            }
            .math {
                color: #0066cc;
                font-weight: bold;
            }
            h1, h2, h3, h4, h5, h6 {
                color: #2c3e50;
                border-bottom: 1px solid #eee;
                padding-bottom: 5px;
            }
        </style>
        """

    def update_content(self, markdown_text):
        """Update the display with new markdown content"""
        self.current_content = markdown_text
        self._render()

    def append_content(self, markdown_text):
        """Append new content (for streaming)"""
        self.current_content += markdown_text
        self._render()

    def clear_content(self):
        """Clear all content"""
        self.current_content = ""
        self.setHtml("")

    def _render(self):
        """Render markdown to HTML"""
        if not self.current_content.strip():
            self.setHtml("")
            return

        content = self._process_math(self.current_content)
        html_content = self.md.convert(content)

        full_html = f"""
        <!DOCTYPE html>
        <html>
        <head>
            <meta charset="UTF-8">
            {self.css}
            <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
            <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
            <script>
                window.MathJax = {{
                    tex: {{
                        inlineMath: [['$', '$'], ['\\\\(', '\\\\)']],
                        displayMath: [['$$', '$$'], ['\\\\[', '\\\\]']]
                    }}
                }};
            </script>
        </head>
        <body>
            {html_content}
        </body>
        </html>
        """
        self.setHtml(full_html)

    def _process_math(self, text):
        """Basic math processing"""
        text = re.sub(r"\$\$(.*?)\$\$", r'<div class="math">\\[\1\\]</div>', text, flags=re.DOTALL)
        text = re.sub(r"\$(.*?)\$", r'<span class="math">\\(\1\\)</span>', text)
        return text


class ConversationListWidget(QListWidget):
    """Widget for displaying conversation history"""

    def __init__(self, config_manager):
        super().__init__()
        self.config_manager = config_manager
        self.refresh_conversations()

    def refresh_conversations(self):
        """Refresh the conversation list"""
        self.clear()
        conversations = self.config_manager.load_conversations()

        for conv in conversations[:50]:  # Show latest 50 conversations
            timestamp = datetime.fromisoformat(conv["timestamp"])
            preview = conv["messages"][0]["content"][:50] + "..." if conv["messages"] else "Empty conversation"

            item_text = f"{timestamp.strftime('%Y-%m-%d %H:%M')} - {preview}"
            item = QListWidgetItem(item_text)
            item.setData(Qt.UserRole, conv)
            self.addItem(item)


class OllamaRAGGui(QMainWindow):
    def __init__(self):
        super().__init__()
        self.setWindowTitle("Cortex - Ollama RAG Agent")

        # Configuration management
        self.config_manager = ConfigManager()
        self.load_settings()

        # Status tracking
        self.ollama_running = False
        self.available_models = []
        self.current_conversation = []

        # Status checker
        self.status_checker = OllamaStatusChecker()
        self.status_checker.status_changed.connect(self.on_ollama_status_changed)
        self.status_checker.models_updated.connect(self.on_models_updated)
        self.status_checker.connection_lost.connect(self.on_connection_lost)

        self.current_worker = None
        self.setup_ui()

        # Start status monitoring
        self.status_checker.start()

    def load_settings(self):
        """Load application settings"""
        width = self.config_manager.get("window.width", 1400)
        height = self.config_manager.get("window.height", 900)
        x = self.config_manager.get("window.x", 100)
        y = self.config_manager.get("window.y", 100)
        self.setGeometry(x, y, width, height)

        self.ollama_url = self.config_manager.get("ollama.url", "http://localhost:11434")
        self.stream_enabled = self.config_manager.get("ui.stream_enabled", True)

    def save_settings(self):
        """Save application settings"""
        geometry = self.geometry()
        self.config_manager.set("window.width", geometry.width())
        self.config_manager.set("window.height", geometry.height())
        self.config_manager.set("window.x", geometry.x())
        self.config_manager.set("window.y", geometry.y())

        if hasattr(self, "stream_button"):
            self.config_manager.set("ui.stream_enabled", self.stream_button.isChecked())

        self.config_manager.save_config()

    def setup_ui(self):
        """Setup the user interface"""
        central_widget = QWidget()
        self.setCentralWidget(central_widget)

        # Main horizontal layout
        main_layout = QHBoxLayout(central_widget)

        # Left sidebar for conversations and controls
        left_panel = QWidget()
        left_panel.setMaximumWidth(300)
        left_layout = QVBoxLayout(left_panel)

        # Status bar
        status_frame = QFrame()
        status_frame.setFrameStyle(QFrame.StyledPanel)
        status_frame.setMaximumHeight(40)
        status_layout = QHBoxLayout(status_frame)

        self.status_label = QLabel("Checking Ollama status...")
        self.status_label.setStyleSheet("color: orange; font-weight: bold;")
        status_layout.addWidget(QLabel("Status:"))
        status_layout.addWidget(self.status_label)
        status_layout.addStretch()

        left_layout.addWidget(status_frame)

        # Model configuration button
        model_config_btn = QPushButton("Configure Models")
        model_config_btn.clicked.connect(self.open_model_config)
        left_layout.addWidget(model_config_btn)

        # Prompt configuration button
        prompt_config_btn = QPushButton("Configure Prompts")
        prompt_config_btn.clicked.connect(self.open_prompt_config)
        left_layout.addWidget(prompt_config_btn)

        # Current model display
        current_model_group = QGroupBox("Current Chat Model")
        current_model_layout = QVBoxLayout(current_model_group)
        self.current_model_label = QLabel("No model selected")
        self.current_model_label.setStyleSheet("font-weight: bold;")
        current_model_layout.addWidget(self.current_model_label)
        left_layout.addWidget(current_model_group)

        # Stream toggle
        self.stream_button = QPushButton("Stream: ON")
        self.stream_button.setCheckable(True)
        self.stream_button.setChecked(self.stream_enabled)
        self.stream_button.clicked.connect(self.toggle_stream)
        self.stream_button.setEnabled(False)
        self.stream_button.setText(f"Stream: {'ON' if self.stream_enabled else 'OFF'}")
        left_layout.addWidget(self.stream_button)

        # Conversation history
        conv_group = QGroupBox("Conversation History")
        conv_layout = QVBoxLayout(conv_group)

        self.conversation_list = ConversationListWidget(self.config_manager)
        self.conversation_list.itemClicked.connect(self.load_conversation)
        conv_layout.addWidget(self.conversation_list)

        new_conv_btn = QPushButton("New Conversation")
        new_conv_btn.clicked.connect(self.new_conversation)
        conv_layout.addWidget(new_conv_btn)

        left_layout.addWidget(conv_group)
        left_layout.addStretch()

        main_layout.addWidget(left_panel)

        # Right panel for chat interface
        right_panel = QWidget()
        right_layout = QVBoxLayout(right_panel)

        # Main content area with splitter
        splitter = QSplitter(Qt.Vertical)

        # Input area
        input_widget = QWidget()
        input_layout = QVBoxLayout(input_widget)
        input_layout.addWidget(QLabel("Input:"))

        self.input_text = QTextEdit()
        self.input_text.setMaximumHeight(150)
        self.input_text.setPlaceholderText("Enter your message here...")
        input_layout.addWidget(self.input_text)

        # Send button
        button_layout = QHBoxLayout()
        self.send_button = QPushButton("Send")
        self.send_button.clicked.connect(self.send_message)
        self.send_button.setEnabled(False)
        button_layout.addWidget(self.send_button)
        button_layout.addStretch()
        input_layout.addLayout(button_layout)

        splitter.addWidget(input_widget)

        # Output area with tabs
        self.output_tabs = QTabWidget()

        # Rendered markdown tab
        self.markdown_display = MarkdownDisplay()
        self.output_tabs.addTab(self.markdown_display, "Rendered")

        # Raw text tab
        self.raw_output = QTextEdit()
        self.raw_output.setReadOnly(True)
        self.raw_output.setFont(QFont("Consolas", 10))
        self.output_tabs.addTab(self.raw_output, "Raw Text")

        # JSON tab (for debugging)
        self.json_output = QTextEdit()
        self.json_output.setReadOnly(True)
        self.json_output.setFont(QFont("Consolas", 10))
        self.output_tabs.addTab(self.json_output, "Debug")

        splitter.addWidget(self.output_tabs)
        splitter.setSizes([200, 600])

        right_layout.addWidget(splitter)

        # Progress bar
        self.progress_bar = QProgressBar()
        self.progress_bar.setVisible(False)
        right_layout.addWidget(self.progress_bar)

        main_layout.addWidget(right_panel)

        # Update UI state
        self.update_model_display()

    def update_model_display(self):
        """Update the current model display"""
        chat_model = self.config_manager.get("ollama.models.chat", "")
        if chat_model:
            if chat_model in self.available_models:
                self.current_model_label.setText(chat_model)
                self.current_model_label.setStyleSheet("font-weight: bold; color: green;")
            else:
                self.current_model_label.setText(f"{chat_model} (unavailable)")
                self.current_model_label.setStyleSheet("font-weight: bold; color: red;")
        else:
            self.current_model_label.setText("No model selected")
            self.current_model_label.setStyleSheet("font-weight: bold; color: orange;")

    def open_model_config(self):
        """Open model configuration dialog"""
        if not self.available_models:
            QMessageBox.warning(
                self,
                "No Models",
                "No Ollama models are available. Please ensure Ollama is running and has models installed.",
            )
            return

        dialog = ModelConfigDialog(self.config_manager, self.available_models, self)
        if dialog.exec() == QDialog.Accepted:
            selected_models = dialog.get_selected_models()
            for task, model in selected_models.items():
                self.config_manager.set(f"ollama.models.{task}", model)
            self.config_manager.save_config()
            self.update_model_display()

    def open_prompt_config(self):
        """Open prompt configuration dialog"""
        dialog = PromptConfigDialog(self.config_manager, self)
        if dialog.exec() == QDialog.Accepted:
            prompts = dialog.get_prompts()
            for prompt_key, prompt_text in prompts.items():
                self.config_manager.set(f"prompts.{prompt_key}", prompt_text)
            self.config_manager.save_config()

    def new_conversation(self):
        """Start a new conversation"""
        if self.current_conversation and self.config_manager.get("ui.auto_save_conversations", True):
            chat_model = self.config_manager.get("ollama.models.chat", "unknown")
            self.config_manager.save_conversation(self.current_conversation, chat_model)

        self.current_conversation = []
        self.markdown_display.clear_content()
        self.raw_output.clear()
        self.json_output.clear()
        self.conversation_list.refresh_conversations()

    def load_conversation(self, item):
        """Load a conversation from history"""
        conv_data = item.data(Qt.UserRole)
        if conv_data and "messages" in conv_data:
            # Display the conversation
            content = ""
            for msg in conv_data["messages"]:
                role = msg.get("role", "unknown")
                text = msg.get("content", "")
                content += f"**{role.title()}:** {text}\n\n"

            self.markdown_display.update_content(content)
            self.current_conversation = conv_data["messages"].copy()

    def on_ollama_status_changed(self, is_running, status_message):
        """Handle Ollama status changes"""
        self.ollama_running = is_running
        self.status_label.setText(status_message)

        if is_running:
            self.status_label.setStyleSheet("color: green; font-weight: bold;")
            chat_model = self.config_manager.get("ollama.models.chat", "")
            self.send_button.setEnabled(bool(chat_model and chat_model in self.available_models))
            self.stream_button.setEnabled(True)
        else:
            self.status_label.setStyleSheet("color: red; font-weight: bold;")
            self.send_button.setEnabled(False)
            self.stream_button.setEnabled(False)

    def on_models_updated(self, models):
        """Handle updated model list from Ollama"""
        self.available_models = models
        self.update_model_display()

        # Enable send button if chat model is available
        chat_model = self.config_manager.get("ollama.models.chat", "")
        if chat_model and chat_model in models and self.ollama_running:
            self.send_button.setEnabled(True)

    def on_connection_lost(self):
        """Handle connection loss - restart polling"""
        self.status_checker.force_model_refresh()

    def toggle_stream(self):
        """Toggle streaming mode"""
        is_streaming = self.stream_button.isChecked()
        self.stream_button.setText(f"Stream: {'ON' if is_streaming else 'OFF'}")

    def send_message(self):
        """Send message to Ollama"""
        if not self.ollama_running:
            self.markdown_display.update_content(
                "**Error:** Ollama is not running. Please start Ollama and wait for connection."
            )
            return

        message = self.input_text.toPlainText().strip()
        if not message:
            return

        chat_model = self.config_manager.get("ollama.models.chat", "")
        if not chat_model:
            self.markdown_display.update_content("**Error:** No chat model configured. Please configure models first.")
            return

        if chat_model not in self.available_models:
            self.markdown_display.update_content(
                f"**Error:** Model '{chat_model}' is not available. Please check your model configuration."
            )
            return

        stream = self.stream_button.isChecked()
        system_prompt = self.config_manager.get("prompts.chat", "")

        # Add user message to conversation
        self.current_conversation.append({"role": "user", "content": message})

        # Clear previous output and show user message
        self.markdown_display.update_content(f"**You:** {message}\n\n**Assistant:** ")
        self.raw_output.clear()
        self.json_output.clear()

        # Clear input
        self.input_text.clear()

        # Show progress
        self.progress_bar.setVisible(True)
        self.progress_bar.setRange(0, 0)
        self.send_button.setEnabled(False)

        # Start worker thread
        self.current_worker = OllamaWorker(chat_model, message, stream, system_prompt)
        self.current_worker.response_received.connect(self.on_response_received)
        self.current_worker.error_occurred.connect(self.on_error)
        self.current_worker.stream_chunk.connect(self.on_stream_chunk)
        self.current_worker.start()

    def on_stream_chunk(self, chunk):
        """Handle streaming chunk"""
        self.markdown_display.append_content(chunk)

        # Update raw text
        self.raw_output.moveCursor(QTextCursor.End)
        self.raw_output.insertPlainText(chunk)

    def on_response_received(self, response):
        """Handle complete response"""
        # Add assistant response to conversation
        self.current_conversation.append({"role": "assistant", "content": response})

        # Auto-save if enabled
        if self.config_manager.get("ui.auto_save_conversations", True):
            chat_model = self.config_manager.get("ollama.models.chat", "")
            self.config_manager.save_conversation(self.current_conversation, chat_model)
            self.conversation_list.refresh_conversations()

        self.json_output.setPlainText(
            f"Response length: {len(response)} characters\nModel: {self.config_manager.get('ollama.models.chat', 'unknown')}"
        )

        # Hide progress and re-enable send
        self.progress_bar.setVisible(False)
        chat_model = self.config_manager.get("ollama.models.chat", "")
        self.send_button.setEnabled(bool(chat_model and chat_model in self.available_models))

    def on_error(self, error):
        """Handle error"""
        self.markdown_display.append_content(f"\n\n**Error:** {error}")
        self.json_output.setPlainText(f"Error: {error}")

        # Force model refresh on connection errors
        if "Connection" in error or "HTTP" in error:
            self.status_checker.force_model_refresh()

        # Hide progress and re-enable send
        self.progress_bar.setVisible(False)
        chat_model = self.config_manager.get("ollama.models.chat", "")
        self.send_button.setEnabled(bool(chat_model and chat_model in self.available_models))

    def closeEvent(self, event):
        """Handle application close event"""
        # Save current conversation if it exists
        if self.current_conversation and self.config_manager.get("ui.auto_save_conversations", True):
            chat_model = self.config_manager.get("ollama.models.chat", "unknown")
            self.config_manager.save_conversation(self.current_conversation, chat_model)

        # Stop status checker
        if hasattr(self, "status_checker"):
            self.status_checker.stop()
            self.status_checker.wait(1000)

        # Save settings
        self.save_settings()

        event.accept()

class OllamaRAGGuiExtended(OllamaRAGGui):
    """Extended GUI with iterative chat support"""

    def setup_ui(self):
        """Extended UI setup with analysis display"""
        # Call the parent setup_ui first
        super().setup_ui()

        # Add the analysis display to the right panel
        # Find the right_layout (this assumes you can access it)
        right_panel = self.centralWidget().layout().itemAt(1).widget()
        right_layout = right_panel.layout()

        # Insert analysis widget before the splitter
        self.analysis_display = AnalysisDisplayWidget()
        right_layout.insertWidget(0, self.analysis_display)

        # Add toggle for iterative mode
        left_panel = self.centralWidget().layout().itemAt(0).widget()
        left_layout = left_panel.layout()

        # Insert after the stream button
        self.iterative_button = QPushButton("Iterative Mode: ON")
        self.iterative_button.setCheckable(True)
        self.iterative_button.setChecked(True)
        self.iterative_button.clicked.connect(self.toggle_iterative_mode)

        # Find the stream button position and insert after it
        for i in range(left_layout.count()):
            widget = left_layout.itemAt(i).widget()
            if hasattr(widget, "text") and "Stream:" in widget.text():
                left_layout.insertWidget(i + 1, self.iterative_button)
                break

    def toggle_iterative_mode(self):
        """Toggle iterative processing mode"""
        is_iterative = self.iterative_button.isChecked()
        self.iterative_button.setText(f"Iterative Mode: {'ON' if is_iterative else 'OFF'}")

        # Show/hide analysis display
        self.analysis_display.setVisible(is_iterative)

    def send_message(self):
        """Enhanced send_message with iterative processing"""
        if not self.ollama_running:
            self.markdown_display.update_content(
                "**Error:** Ollama is not running. Please start Ollama and wait for connection."
            )
            return

        message = self.input_text.toPlainText().strip()
        if not message:
            return

        chat_model = self.config_manager.get("ollama.models.chat", "")
        if not chat_model:
            self.markdown_display.update_content("**Error:** No chat model configured. Please configure models first.")
            return

        if chat_model not in self.available_models:
            self.markdown_display.update_content(
                f"**Error:** Model '{chat_model}' is not available. Please check your model configuration."
            )
            return

        # Add user message to conversation
        self.current_conversation.append({"role": "user", "content": message})

        # Clear previous output and show user message
        self.markdown_display.update_content(f"**You:** {message}\n\n")
        self.raw_output.clear()
        self.json_output.clear()

        # Clear input
        self.input_text.clear()

        # Show progress
        self.progress_bar.setVisible(True)
        self.progress_bar.setRange(0, 0)
        self.send_button.setEnabled(False)

        # Choose processing mode
        if self.iterative_button.isChecked():
            # Use iterative processing
            self.current_worker = IterativeOllamaWorker(
                self.config_manager,
                message,
                self.current_conversation[:-1],  # Don't include the current message
            )

            # Connect new signals
            self.current_worker.status_updated.connect(self.analysis_display.update_status)
            self.current_worker.analysis_completed.connect(self.analysis_display.update_analysis)

        else:
            # Use original processing
            stream = self.stream_button.isChecked()
            system_prompt = self.config_manager.get("prompts.chat", "")

            self.current_worker = OllamaWorker(chat_model, message, stream, system_prompt)
            self.current_worker.stream_chunk.connect(self.on_stream_chunk)

        # Connect common signals
        self.current_worker.response_received.connect(self.on_response_received)
        self.current_worker.error_occurred.connect(self.on_error)
        self.current_worker.start()

    def on_response_received(self, response):
        """Enhanced response handler"""
        # Update markdown display with assistant response
        current_content = self.markdown_display.current_content
        if not current_content.endswith("**Assistant:** "):
            self.markdown_display.append_content("**Assistant:** ")

        self.markdown_display.append_content(response)

        # Add assistant response to conversation
        self.current_conversation.append({"role": "assistant", "content": response})

        # Auto-save if enabled
        if self.config_manager.get("ui.auto_save_conversations", True):
            chat_model = self.config_manager.get("ollama.models.chat", "")
            self.config_manager.save_conversation(self.current_conversation, chat_model)
            self.conversation_list.refresh_conversations()

        # Update debug info
        model_used = self.config_manager.get("ollama.models.chat", "unknown")
        mode = "Iterative" if self.iterative_button.isChecked() else "Direct"

        self.json_output.setPlainText(
            f"Response length: {len(response)} characters\n"
            f"Model: {model_used}\n"
            f"Mode: {mode}\n"
            f"Conversation length: {len(self.current_conversation)} messages"
        )

        # Hide progress and re-enable send
        self.progress_bar.setVisible(False)
        chat_model = self.config_manager.get("ollama.models.chat", "")
        self.send_button.setEnabled(bool(chat_model and chat_model in self.available_models))

        # Update analysis status
        if hasattr(self, "analysis_display"):
            self.analysis_display.update_status("Complete")


def main():
    app = QApplication(sys.argv)
    app.setStyle("Fusion")

    window = OllamaRAGGuiExtended()
    window.show()

    sys.exit(app.exec())


if __name__ == "__main__":
    main()
